{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87bc0aa6-389b-4411-82be-a32acb5b29d2",
   "metadata": {},
   "source": [
    "# 1.pipeline 可以干什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140a90b-16aa-4ea4-ab6b-e5b51dd20be2",
   "metadata": {},
   "source": [
    "## 文本被预处理为模型可以理解的格式。\n",
    "## 将预处理后的输入传递给模型。\n",
    "## 对模型的预测进行后续处理并输出最终人类可以理解的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301bb60b-6a93-4749-b4c7-51955fe2e9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55a961294804bf6af601e62000b779c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01976f-00a8-4949-b188-e868ae4491b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64735596-09d1-4aab-a5d0-f203cda9d2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fccaa33014748778e05f88ac81bab1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\celia\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c3b6fef13e4ec8afc1f4cb0b0cc797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f978dfa63d54906a4f89c20d0e6ac3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cc5f0b1edf4f288e916f40c765890a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998764991760254}]\n"
     ]
    }
   ],
   "source": [
    "# 指定模型名称\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# 测试\n",
    "print(classifier(\"I love this!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b93642f4-a4db-4e3c-acf6-d6fd513d0f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598045349121094}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#语句情感分析\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fae79f-46bb-4cfc-91ba-8435951145b2",
   "metadata": {},
   "source": [
    "# 2.pipeline用法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda7165-508a-4cfa-be55-d65fd398628a",
   "metadata": {},
   "source": [
    "# 2.1零样本分类：我们需要对没有标签的文本进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d86f1fb-6e0a-4b62-9b53-720d1f448db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli and revision 130fb28 (https://huggingface.co/roberta-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879324f0cbda42d6845627277fbb9e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\celia\\.cache\\huggingface\\hub\\models--roberta-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fa4d79600c460e89df7864046ffda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4620f7f21a6146a382aec637c2b1c738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1f1e9353c344a4b887b3bcbaa928b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32c3094aee94e72a4645aac98b1e637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6038f8a302fc45b59a7119266ab3ddd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'will triump win the election?',\n",
       " 'labels': ['politics', 'education', 'business'],\n",
       " 'scores': [0.9739829897880554, 0.013753261417150497, 0.012263766489923]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"will triump win the election?\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968a21b-9179-4305-9fb1-60c887b20131",
   "metadata": {},
   "source": [
    "# 2.2文本随机生成text-generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35635b69-cfb8-4ec7-ac2e-95221c5ebd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_return_sequences 控制生成多少个不同的候选的句子\n",
    "#参数 max_length 控制输出文本的总长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d92e513-2441-4526-a91c-83fda2680f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1ac8649d7d4cf98ccad82be0e33ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\celia\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py:991: UserWarning: Not enough free disk space to download the file. The expected file size is: 548.11 MB. The target location C:\\Users\\celia\\.cache\\huggingface\\hub\\models--gpt2\\blobs only has 497.12 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab99b7116a5140a2807683c0155b19a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py:991: UserWarning: Not enough free disk space to download the file. The expected file size is: 548.11 MB. The target location C:\\Users\\celia\\.cache\\huggingface\\hub\\models--gpt2\\blobs only has 0.00 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571d8514ba3a4add8cb6b6ca12b08482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  94%|#########3| 514M/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model gpt2 with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel'>). See the original errors:\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 278, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2794, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\utils\\hub.py\", line 385, in cached_file\n    resolved_file = hf_hub_download(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with TFGPT2LMHeadModel, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 278, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2794, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\utils\\hub.py\", line 385, in cached_file\n    resolved_file = hf_hub_download(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 2\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m generator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn this course, we will teach you how to\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\pipelines\\__init__.py:870\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 870\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    881\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\pipelines\\base.py:291\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    290\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 291\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m         )\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    296\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model gpt2 with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel'>). See the original errors:\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 278, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2794, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\utils\\hub.py\", line 385, in cached_file\n    resolved_file = hf_hub_download(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\nwhile loading with TFGPT2LMHeadModel, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 278, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2794, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\transformers\\utils\\hub.py\", line 385, in cached_file\n    resolved_file = hf_hub_download(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py\", line 101, in inner_f\n    return f(*args, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1240, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1389, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 1915, in _download_to_tmp_and_move\n    http_get(\n  File \"C:\\Users\\celia\\anaconda3\\envs\\NLP_env\\lib\\site-packages\\huggingface_hub\\file_download.py\", line 552, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e5050-daf1-452e-997a-da6077e353ba",
   "metadata": {},
   "source": [
    "# 2.3选用不同模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08556cb-d14a-4ea7-a224-336cb36323d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator = pipeline(\"text-generation\", model=\"distilgpt2\")可以在模型筛选页码查看不同模型并选用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP_env]",
   "language": "python",
   "name": "conda-env-NLP_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
